{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5evNZsjt2WF"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)\n",
        "\n",
        "Modified by uramoon@kw.ac.kr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMtBnW6Vt2WJ"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "감성 분석 (Sentiment Analysis)은 자연어 처리 (Natural Language Processing, NLP)를 이용하여 텍스트의 감정을 파악하는 기술이다. 이 노트북에서는 영화 리뷰가 긍정적인지 부정적인지 분류할 것이다.\n",
        "\n",
        "\"This movie is not very good.\"을 보면 \"very good\"은 긍정적인 감정이지만 \"not\"이 있기 때문에 부정적인 감정으로 분류되어야 한다. 이러한 것을 어떻게 학습시킬 수 있을까?\n",
        "\n",
        "1. 인공 신경망은 숫자를 입력으로 받아들이는데 텍스트를 어떻게 수치 데이터로 변환할지 생각해봐야 한다.\n",
        "2. 문서의 길이는 문서마다 다른데 크기가 각기 다른 입력 데이터를 인공 신경망에 어떻게 입력해야할지 생각해봐야 한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMoL_tZ7t2WJ"
      },
      "source": [
        "## Flowchart\n",
        "\n",
        "1. Tokenizer를 이용하여 각 단어를 정수로 변환한다. 예) the: 1, and: 2, a: 3, ...\n",
        "2. 임베딩 (embedding)을 통해 각 정수를 n차원 벡터로 변환한다. (가까운 단어는 가깝게 위치)\n",
        "3. 문서를 순환 신경망 (recurrent neural network, RNN)에 입력하여 0 (부정적)부터 1 (긍적적) 사이의 실수를 출력하게 훈련한다.\n",
        "\n",
        "The flowchart of the algorithm is roughly:\n",
        "\n",
        "<img src=\"https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_natural_language_flowchart.png?raw=1\" alt=\"Flowchart NLP\" style=\"width: 300px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdGt4vEDt2WK"
      },
      "source": [
        "## Recurrent Neural Network (RNN)\n",
        "\n",
        "RNN의 기본 유닛은 Recurrent Unit (RU)으로 LSTM (Long-Short-Term-Memory)과 성능 하락을 최소화하면서 LSTM을 단순화한 GRU (Gated Recurrent Unit)가 많이 사용된다.\n",
        "\n",
        "RU는 과거의 상태를 기억하고 있다가 현재의 입력에 대해 자신의 상태를 바꾸면서 값을 출력한다.\n",
        "\n",
        "새로운 상태는 과거의 상태와 현재의 입력에 따라 결정된다. 예를 들어 최근의 입력에 \"not\"이 있었고, 현재의 입력이 \"good\"이라면 새로운 상태는 \"not good\"에 해당하는 부정적인 감정을 기억할 것이다. \n",
        "\n",
        "![Recurrent unit](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_recurrent_unit.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTHceb8_t2WK"
      },
      "source": [
        "### Unrolled Network\n",
        "\n",
        "RNN에서 RU가 다음 입력에 과거의 정보를 전달하는 과정을 다음과 같이 펼쳐서 도식화할 수 있다. \n",
        "\n",
        "RU의 메모리는 0으로 초기화된다.\n",
        "\n",
        "가장 처음 \"this\"가 입력되면 메모리는 새로운 상태를 저장하고 무언가를 출력하지만 출력값을 사용하지는 않는다. 글을 끝까지 읽었을 때의 출력값만 활용할 것이다.\n",
        "\n",
        "두 번째 단어는 \"is\"인데 바로 전에 읽은 \"this\"와 결합하여 새로운 상태를 저장한다.\n",
        "\n",
        "세 번째 단어는 \"not\"인데 \"not\"을 기억하고 있다가 나중에 \"good\"을 읽었을 때 그것을 부정적인 단어로 바꿔주는 역할을 할 것이다. \n",
        "\n",
        "문서의 마지막 단어를 읽었을 때 0부터 1사이의 값을 출력하는데 이를 활용하여 감성 분석을 수행한다.\n",
        "\n",
        "Note that for the sake of clarity, this figure doesn't show the mapping from text-words to integer-tokens and embedding-vectors, as well as the fully-connected Sigmoid layer on the output.\n",
        "\n",
        "![Unrolled network](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/images/20_unrolled_flowchart.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib5anMcIt2WL"
      },
      "source": [
        "### Exploding & Vanishing Gradients\n",
        "\n",
        "매 스텝 새로운 단어가 들어올 때마다 내부 상태를 변경할 경우 기울기 소실 혹은 폭주 문제가 발생할 수 있습니다.\n",
        "\n",
        "하나의 텍스트가 500개의 단어로 구성된다고 했을 때 내부 상태가 500번 변화하게 되는데 각 변화마다 기울기를 곱할 때 기울기가 1 미만이면 그 값이 0에 가까워지고, 기울기가 1보다 크면 그 값은 매우 커집니다.\n",
        "\n",
        "이러한 기울기 소실 / 폭주 문제를 해결하기 위해 매 입력마다 상태가 변화하지 않는 능력을 지닌 장기기억 메모리를 도입한 것이 LSTM과 GRU입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqee1-G7BMYi"
      },
      "source": [
        "## 준비 단계\n",
        "1. 런타임을 GPU로 설정해주세요.\n",
        "2. imdb.py와 download.py 파일을 Colab 환경에 복사해주세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwiNseb1t2WM"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NACTD0ZTt2WM"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsG8d_Kst2WN"
      },
      "source": [
        "We need to import several things from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1NoU7t1Ht2WO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiZrp2Ppt2WP"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "IMDB에서 50,000개의 영화 리뷰를 다운받습니다. Keras에 정제된 IMDB 데이터셋이 있지만 단어를 정수로 변환하는 작업을 직접 수행하기 위해 정제되지 않은 데이터셋을 사용합니다. 50,000개 중 25,000개는 훈련 데이터, 나머지 25,000개는 테스트 데이터입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RZWc7HVCt2WP"
      },
      "outputs": [],
      "source": [
        "import imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwqtMAtt2WQ"
      },
      "source": [
        "Automatically download and extract the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShNhaNQ4t2WQ",
        "outputId": "ea04d187-2fa1-45e2-c17a-d9d3852f893d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Download progress: 100.0%\n",
            "Download finished. Extracting files.\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "imdb.maybe_download_and_extract()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBXbQObrB5lv"
      },
      "source": [
        "## 다운받은 파일 확인\n",
        "폴더를 새로고침하면 다운로드 받은 data 폴더가 보입니다.\n",
        "\n",
        "1. 훈련 데이터 폴더: data/IMDB/aclImdb/train/ \n",
        "2. 테스트 데이터 폴더: data/IMDB/aclImdb/test/ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KctXlcN9B4Nn",
        "outputId": "7836840b-2811-4d09-9a37-2f7f25645385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often)."
          ]
        }
      ],
      "source": [
        "# 훈련 데이터의 pos 폴더에서 10점짜리 긍정적 리뷰 확인\n",
        "#  This movie gets better each time I see it (which is quite often).\n",
        "!cat data/IMDB/aclImdb/train/pos/10001_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0DdsDcDDDDO",
        "outputId": "bb6eb7c2-66a8-4f10-a555-0fb4cf7e2e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying."
          ]
        }
      ],
      "source": [
        "# 훈련 데이터의 neg 폴더에서 1점짜리 부정적 리뷰 확인 \n",
        "# this story is too painful to watch. \n",
        "!cat data/IMDB/aclImdb/train/neg/10002_1.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI5HZG9ut2WQ"
      },
      "source": [
        "## 훈련 데이터와 테스트 데이터 만들기\n",
        "x 값에는 리뷰 내용이 (텍스트), y 값에는 0 (부정) 혹은 1 (긍정)이 기록됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "anIzfJ5-t2WQ"
      },
      "outputs": [],
      "source": [
        "x_train_text, y_train = imdb.load_data(train=True)\n",
        "x_test_text, y_test = imdb.load_data(train=False)\n",
        "\n",
        "# Convert to numpy arrays.\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-fgoTa9t2WR",
        "outputId": "2304ff55-73b8-4113-d0b9-09cca386d649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train-set size:  25000\n",
            "Test-set size:   25000\n"
          ]
        }
      ],
      "source": [
        "print(\"Train-set size: \", len(x_train_text))\n",
        "print(\"Test-set size:  \", len(x_test_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI6MDlNIEVng",
        "outputId": "bf55203b-77fb-4cb1-9500-775f373a152e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "리뷰: Hmmmm, want a little romance with your mystery? This has it. I think if the romance was ditched this would have made for a better movie. But how could the romance be ditched when the story's borrowed from something called a Harlequin Romance novel, whatever the heck that is. Had the romance been ditched, the story might have been a little too weak. The mystery here wasn't too bad, quite interesting but nothing on the level of Mission Impossible international espionage. Oh well. I thought Mel Harris was pretty good; her short skirts, i think, added some sex appeal... but this Rob Stewart guy probably could have been better cast, maybe with a more well known TV movie actor. The directing was decent and the writing could have been improved on - both could have been a little edgier, a little darker, more adventurous. One thing that was great about this was the use of real European locations. That could easily have been changed so this could have been filmed in Canada but they really were in magnificently beautiful places like Budapest. Possibly a drawback was the director and/or cinematographer's choice to frame certain shots picture postcard perfect. Not good. Had this been a more dramatic motion picture shot for the big screen, picture postcard perfect scenes really need to take a backseat and just be a nice part of the background. This was just a tv-movie, though, so they had to add some Ummmph to the picture and some of that Ummmph came from the scenery. Overall, twasn't really a bad movie. I'll tell you what, this was absolutely the best Canadian-Hungarian production I have ever seen! (and the only that i know of.) I hereby proclaim this to be a mediocre made-for-tv movie, giving it a grade of C-\n",
            "정답: 1.0\n"
          ]
        }
      ],
      "source": [
        "# TODO: 훈련데이터의 0번째 x값(리뷰)과 y값(정답)을 출력해보세요.\n",
        "print(\"리뷰:\", x_train_text[0])\n",
        "print(\"정답:\", y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAD8-TAWt2WS"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "인공신경망은 숫자를 입력받기 때문에 우선 각 단어를 tokenizer를 통해 정수로 변환합니다.<br>Tokenizer는 데이터셋에서 가장 많이 등장하는 n개의 단어를 정수로 변환합니다.<br> 예) the: 1, and: 2, a: 3, ... \n",
        "<br>\n",
        "일단 10000개의 단어만 사용하여 모델을 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UZtN0bXxt2WS"
      },
      "outputs": [],
      "source": [
        "num_words = 10000\n",
        "tokenizer = Tokenizer(num_words=num_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEZUMyLIt2WS"
      },
      "source": [
        "훈련 데이터와 테스트 데이터에 들어 있는 모든 단어를 변환할 필요가 있기 때문에 훈련 데이터와 테스트 데이터를 함께 tokenizer에 입력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qXkL1ONt2WS",
        "outputId": "c90fe1b2-a40c-4430-93ac-3a7083b3595b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.56 s, sys: 45.9 ms, total: 7.61 s\n",
            "Wall time: 7.73 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "data_text = x_train_text + x_test_text\n",
        "tokenizer.fit_on_texts(data_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvJr9Xfyt2WT"
      },
      "source": [
        "Tokenizer가 찾아낸 많이 등장하는 단어들입니다. 해당 단어들은 정수로 변환되고 나머지 단어들은 제거될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU5AyhO8t2WT",
        "outputId": "d2bc3d53-caa6-4757-91b5-b9e812243137"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'the': 1,\n",
              " 'and': 2,\n",
              " 'a': 3,\n",
              " 'of': 4,\n",
              " 'to': 5,\n",
              " 'is': 6,\n",
              " 'br': 7,\n",
              " 'in': 8,\n",
              " 'it': 9,\n",
              " 'i': 10,\n",
              " 'this': 11,\n",
              " 'that': 12,\n",
              " 'was': 13,\n",
              " 'as': 14,\n",
              " 'for': 15,\n",
              " 'with': 16,\n",
              " 'movie': 17,\n",
              " 'but': 18,\n",
              " 'film': 19,\n",
              " 'on': 20,\n",
              " 'not': 21,\n",
              " 'you': 22,\n",
              " 'are': 23,\n",
              " 'his': 24,\n",
              " 'have': 25,\n",
              " 'be': 26,\n",
              " 'one': 27,\n",
              " 'he': 28,\n",
              " 'all': 29,\n",
              " 'at': 30,\n",
              " 'by': 31,\n",
              " 'an': 32,\n",
              " 'they': 33,\n",
              " 'so': 34,\n",
              " 'who': 35,\n",
              " 'from': 36,\n",
              " 'like': 37,\n",
              " 'or': 38,\n",
              " 'just': 39,\n",
              " 'her': 40,\n",
              " 'out': 41,\n",
              " 'about': 42,\n",
              " 'if': 43,\n",
              " \"it's\": 44,\n",
              " 'has': 45,\n",
              " 'there': 46,\n",
              " 'some': 47,\n",
              " 'what': 48,\n",
              " 'good': 49,\n",
              " 'when': 50,\n",
              " 'more': 51,\n",
              " 'very': 52,\n",
              " 'up': 53,\n",
              " 'no': 54,\n",
              " 'time': 55,\n",
              " 'my': 56,\n",
              " 'even': 57,\n",
              " 'would': 58,\n",
              " 'she': 59,\n",
              " 'which': 60,\n",
              " 'only': 61,\n",
              " 'really': 62,\n",
              " 'see': 63,\n",
              " 'story': 64,\n",
              " 'their': 65,\n",
              " 'had': 66,\n",
              " 'can': 67,\n",
              " 'me': 68,\n",
              " 'well': 69,\n",
              " 'were': 70,\n",
              " 'than': 71,\n",
              " 'much': 72,\n",
              " 'we': 73,\n",
              " 'bad': 74,\n",
              " 'been': 75,\n",
              " 'get': 76,\n",
              " 'do': 77,\n",
              " 'great': 78,\n",
              " 'other': 79,\n",
              " 'will': 80,\n",
              " 'also': 81,\n",
              " 'into': 82,\n",
              " 'people': 83,\n",
              " 'because': 84,\n",
              " 'how': 85,\n",
              " 'first': 86,\n",
              " 'him': 87,\n",
              " 'most': 88,\n",
              " \"don't\": 89,\n",
              " 'made': 90,\n",
              " 'then': 91,\n",
              " 'its': 92,\n",
              " 'them': 93,\n",
              " 'make': 94,\n",
              " 'way': 95,\n",
              " 'too': 96,\n",
              " 'movies': 97,\n",
              " 'could': 98,\n",
              " 'any': 99,\n",
              " 'after': 100,\n",
              " 'think': 101,\n",
              " 'characters': 102,\n",
              " 'watch': 103,\n",
              " 'films': 104,\n",
              " 'two': 105,\n",
              " 'many': 106,\n",
              " 'seen': 107,\n",
              " 'character': 108,\n",
              " 'being': 109,\n",
              " 'never': 110,\n",
              " 'plot': 111,\n",
              " 'love': 112,\n",
              " 'acting': 113,\n",
              " 'life': 114,\n",
              " 'did': 115,\n",
              " 'best': 116,\n",
              " 'where': 117,\n",
              " 'know': 118,\n",
              " 'show': 119,\n",
              " 'little': 120,\n",
              " 'over': 121,\n",
              " 'off': 122,\n",
              " 'ever': 123,\n",
              " 'does': 124,\n",
              " 'your': 125,\n",
              " 'better': 126,\n",
              " 'end': 127,\n",
              " 'man': 128,\n",
              " 'scene': 129,\n",
              " 'still': 130,\n",
              " 'say': 131,\n",
              " 'these': 132,\n",
              " 'here': 133,\n",
              " 'scenes': 134,\n",
              " 'why': 135,\n",
              " 'while': 136,\n",
              " 'something': 137,\n",
              " 'such': 138,\n",
              " 'go': 139,\n",
              " 'through': 140,\n",
              " 'back': 141,\n",
              " 'should': 142,\n",
              " 'those': 143,\n",
              " 'real': 144,\n",
              " \"i'm\": 145,\n",
              " 'now': 146,\n",
              " 'watching': 147,\n",
              " 'thing': 148,\n",
              " \"doesn't\": 149,\n",
              " 'actors': 150,\n",
              " 'though': 151,\n",
              " 'funny': 152,\n",
              " 'years': 153,\n",
              " \"didn't\": 154,\n",
              " 'old': 155,\n",
              " 'another': 156,\n",
              " '10': 157,\n",
              " 'work': 158,\n",
              " 'before': 159,\n",
              " 'actually': 160,\n",
              " 'nothing': 161,\n",
              " 'makes': 162,\n",
              " 'look': 163,\n",
              " 'director': 164,\n",
              " 'find': 165,\n",
              " 'going': 166,\n",
              " 'same': 167,\n",
              " 'new': 168,\n",
              " 'lot': 169,\n",
              " 'every': 170,\n",
              " 'few': 171,\n",
              " 'again': 172,\n",
              " 'part': 173,\n",
              " 'cast': 174,\n",
              " 'down': 175,\n",
              " 'us': 176,\n",
              " 'things': 177,\n",
              " 'want': 178,\n",
              " 'quite': 179,\n",
              " 'pretty': 180,\n",
              " 'world': 181,\n",
              " 'horror': 182,\n",
              " 'around': 183,\n",
              " 'seems': 184,\n",
              " \"can't\": 185,\n",
              " 'young': 186,\n",
              " 'take': 187,\n",
              " 'however': 188,\n",
              " 'got': 189,\n",
              " 'thought': 190,\n",
              " 'big': 191,\n",
              " 'fact': 192,\n",
              " 'enough': 193,\n",
              " 'long': 194,\n",
              " 'both': 195,\n",
              " \"that's\": 196,\n",
              " 'give': 197,\n",
              " \"i've\": 198,\n",
              " 'own': 199,\n",
              " 'may': 200,\n",
              " 'between': 201,\n",
              " 'comedy': 202,\n",
              " 'right': 203,\n",
              " 'series': 204,\n",
              " 'action': 205,\n",
              " 'must': 206,\n",
              " 'music': 207,\n",
              " 'without': 208,\n",
              " 'times': 209,\n",
              " 'saw': 210,\n",
              " 'always': 211,\n",
              " 'original': 212,\n",
              " \"isn't\": 213,\n",
              " 'role': 214,\n",
              " 'come': 215,\n",
              " 'almost': 216,\n",
              " 'gets': 217,\n",
              " 'interesting': 218,\n",
              " 'guy': 219,\n",
              " 'point': 220,\n",
              " 'done': 221,\n",
              " \"there's\": 222,\n",
              " 'whole': 223,\n",
              " 'least': 224,\n",
              " 'far': 225,\n",
              " 'bit': 226,\n",
              " 'script': 227,\n",
              " 'minutes': 228,\n",
              " 'feel': 229,\n",
              " '2': 230,\n",
              " 'anything': 231,\n",
              " 'making': 232,\n",
              " 'might': 233,\n",
              " 'since': 234,\n",
              " 'am': 235,\n",
              " 'family': 236,\n",
              " \"he's\": 237,\n",
              " 'last': 238,\n",
              " 'probably': 239,\n",
              " 'tv': 240,\n",
              " 'performance': 241,\n",
              " 'kind': 242,\n",
              " 'away': 243,\n",
              " 'yet': 244,\n",
              " 'fun': 245,\n",
              " 'worst': 246,\n",
              " 'sure': 247,\n",
              " 'rather': 248,\n",
              " 'hard': 249,\n",
              " 'anyone': 250,\n",
              " 'girl': 251,\n",
              " 'each': 252,\n",
              " 'played': 253,\n",
              " 'day': 254,\n",
              " 'found': 255,\n",
              " 'looking': 256,\n",
              " 'woman': 257,\n",
              " 'screen': 258,\n",
              " 'although': 259,\n",
              " 'our': 260,\n",
              " 'especially': 261,\n",
              " 'believe': 262,\n",
              " 'having': 263,\n",
              " 'trying': 264,\n",
              " 'course': 265,\n",
              " 'dvd': 266,\n",
              " 'everything': 267,\n",
              " 'set': 268,\n",
              " 'goes': 269,\n",
              " 'comes': 270,\n",
              " 'put': 271,\n",
              " 'ending': 272,\n",
              " 'maybe': 273,\n",
              " 'place': 274,\n",
              " 'book': 275,\n",
              " 'shows': 276,\n",
              " 'three': 277,\n",
              " 'worth': 278,\n",
              " 'different': 279,\n",
              " 'main': 280,\n",
              " 'once': 281,\n",
              " 'sense': 282,\n",
              " 'american': 283,\n",
              " 'reason': 284,\n",
              " 'looks': 285,\n",
              " 'effects': 286,\n",
              " 'watched': 287,\n",
              " 'play': 288,\n",
              " 'true': 289,\n",
              " 'money': 290,\n",
              " 'actor': 291,\n",
              " \"wasn't\": 292,\n",
              " 'job': 293,\n",
              " 'together': 294,\n",
              " 'war': 295,\n",
              " 'someone': 296,\n",
              " 'plays': 297,\n",
              " 'instead': 298,\n",
              " 'high': 299,\n",
              " 'during': 300,\n",
              " 'year': 301,\n",
              " 'said': 302,\n",
              " 'half': 303,\n",
              " 'everyone': 304,\n",
              " 'later': 305,\n",
              " 'takes': 306,\n",
              " '1': 307,\n",
              " 'seem': 308,\n",
              " 'audience': 309,\n",
              " 'special': 310,\n",
              " 'beautiful': 311,\n",
              " 'left': 312,\n",
              " 'himself': 313,\n",
              " 'seeing': 314,\n",
              " 'john': 315,\n",
              " 'night': 316,\n",
              " 'black': 317,\n",
              " 'version': 318,\n",
              " 'shot': 319,\n",
              " 'excellent': 320,\n",
              " 'idea': 321,\n",
              " 'house': 322,\n",
              " 'mind': 323,\n",
              " 'star': 324,\n",
              " 'wife': 325,\n",
              " 'fan': 326,\n",
              " 'death': 327,\n",
              " 'used': 328,\n",
              " 'else': 329,\n",
              " 'simply': 330,\n",
              " 'nice': 331,\n",
              " 'budget': 332,\n",
              " 'poor': 333,\n",
              " 'short': 334,\n",
              " 'completely': 335,\n",
              " 'second': 336,\n",
              " \"you're\": 337,\n",
              " '3': 338,\n",
              " 'read': 339,\n",
              " 'along': 340,\n",
              " 'less': 341,\n",
              " 'top': 342,\n",
              " 'help': 343,\n",
              " 'home': 344,\n",
              " 'men': 345,\n",
              " 'either': 346,\n",
              " 'line': 347,\n",
              " 'boring': 348,\n",
              " 'dead': 349,\n",
              " 'friends': 350,\n",
              " 'kids': 351,\n",
              " 'try': 352,\n",
              " 'production': 353,\n",
              " 'enjoy': 354,\n",
              " 'camera': 355,\n",
              " 'use': 356,\n",
              " 'wrong': 357,\n",
              " 'given': 358,\n",
              " 'low': 359,\n",
              " 'classic': 360,\n",
              " 'father': 361,\n",
              " 'need': 362,\n",
              " 'full': 363,\n",
              " 'stupid': 364,\n",
              " 'next': 365,\n",
              " 'until': 366,\n",
              " 'performances': 367,\n",
              " 'school': 368,\n",
              " 'hollywood': 369,\n",
              " 'rest': 370,\n",
              " 'truly': 371,\n",
              " 'awful': 372,\n",
              " 'video': 373,\n",
              " 'couple': 374,\n",
              " 'start': 375,\n",
              " 'sex': 376,\n",
              " 'recommend': 377,\n",
              " 'women': 378,\n",
              " 'let': 379,\n",
              " 'tell': 380,\n",
              " 'terrible': 381,\n",
              " 'remember': 382,\n",
              " 'mean': 383,\n",
              " 'came': 384,\n",
              " 'understand': 385,\n",
              " 'getting': 386,\n",
              " 'perhaps': 387,\n",
              " 'moments': 388,\n",
              " 'name': 389,\n",
              " 'keep': 390,\n",
              " 'face': 391,\n",
              " 'itself': 392,\n",
              " 'wonderful': 393,\n",
              " 'playing': 394,\n",
              " 'human': 395,\n",
              " 'style': 396,\n",
              " 'small': 397,\n",
              " 'episode': 398,\n",
              " 'perfect': 399,\n",
              " 'others': 400,\n",
              " 'person': 401,\n",
              " 'doing': 402,\n",
              " 'often': 403,\n",
              " 'early': 404,\n",
              " 'stars': 405,\n",
              " 'definitely': 406,\n",
              " 'written': 407,\n",
              " 'head': 408,\n",
              " 'lines': 409,\n",
              " 'dialogue': 410,\n",
              " 'gives': 411,\n",
              " 'piece': 412,\n",
              " \"couldn't\": 413,\n",
              " 'went': 414,\n",
              " 'finally': 415,\n",
              " 'mother': 416,\n",
              " 'case': 417,\n",
              " 'title': 418,\n",
              " 'absolutely': 419,\n",
              " 'live': 420,\n",
              " 'boy': 421,\n",
              " 'yes': 422,\n",
              " 'laugh': 423,\n",
              " 'certainly': 424,\n",
              " 'liked': 425,\n",
              " 'become': 426,\n",
              " 'entertaining': 427,\n",
              " 'worse': 428,\n",
              " 'oh': 429,\n",
              " 'sort': 430,\n",
              " 'loved': 431,\n",
              " 'lost': 432,\n",
              " 'called': 433,\n",
              " 'hope': 434,\n",
              " 'picture': 435,\n",
              " 'felt': 436,\n",
              " 'overall': 437,\n",
              " 'entire': 438,\n",
              " 'several': 439,\n",
              " 'mr': 440,\n",
              " 'based': 441,\n",
              " 'supposed': 442,\n",
              " 'cinema': 443,\n",
              " 'friend': 444,\n",
              " 'guys': 445,\n",
              " 'sound': 446,\n",
              " '5': 447,\n",
              " 'problem': 448,\n",
              " 'drama': 449,\n",
              " 'against': 450,\n",
              " 'waste': 451,\n",
              " 'white': 452,\n",
              " 'beginning': 453,\n",
              " '4': 454,\n",
              " 'fans': 455,\n",
              " 'totally': 456,\n",
              " 'dark': 457,\n",
              " 'care': 458,\n",
              " 'direction': 459,\n",
              " 'humor': 460,\n",
              " 'wanted': 461,\n",
              " \"she's\": 462,\n",
              " 'seemed': 463,\n",
              " 'under': 464,\n",
              " 'game': 465,\n",
              " 'children': 466,\n",
              " 'despite': 467,\n",
              " 'lives': 468,\n",
              " 'lead': 469,\n",
              " 'guess': 470,\n",
              " 'example': 471,\n",
              " 'already': 472,\n",
              " 'final': 473,\n",
              " 'throughout': 474,\n",
              " \"you'll\": 475,\n",
              " 'evil': 476,\n",
              " 'turn': 477,\n",
              " 'becomes': 478,\n",
              " 'unfortunately': 479,\n",
              " 'able': 480,\n",
              " 'quality': 481,\n",
              " \"i'd\": 482,\n",
              " 'days': 483,\n",
              " 'history': 484,\n",
              " 'fine': 485,\n",
              " 'side': 486,\n",
              " 'wants': 487,\n",
              " 'heart': 488,\n",
              " 'horrible': 489,\n",
              " 'writing': 490,\n",
              " 'amazing': 491,\n",
              " 'b': 492,\n",
              " 'flick': 493,\n",
              " 'killer': 494,\n",
              " 'run': 495,\n",
              " 'son': 496,\n",
              " '\\x96': 497,\n",
              " 'michael': 498,\n",
              " 'works': 499,\n",
              " 'close': 500,\n",
              " \"they're\": 501,\n",
              " 'act': 502,\n",
              " 'art': 503,\n",
              " 'kill': 504,\n",
              " 'matter': 505,\n",
              " 'etc': 506,\n",
              " 'tries': 507,\n",
              " \"won't\": 508,\n",
              " 'past': 509,\n",
              " 'town': 510,\n",
              " 'turns': 511,\n",
              " 'enjoyed': 512,\n",
              " 'brilliant': 513,\n",
              " 'gave': 514,\n",
              " 'behind': 515,\n",
              " 'parts': 516,\n",
              " 'stuff': 517,\n",
              " 'genre': 518,\n",
              " 'eyes': 519,\n",
              " 'car': 520,\n",
              " 'favorite': 521,\n",
              " 'directed': 522,\n",
              " 'late': 523,\n",
              " 'hand': 524,\n",
              " 'expect': 525,\n",
              " 'soon': 526,\n",
              " 'hour': 527,\n",
              " 'obviously': 528,\n",
              " 'themselves': 529,\n",
              " 'sometimes': 530,\n",
              " 'killed': 531,\n",
              " 'actress': 532,\n",
              " 'thinking': 533,\n",
              " 'child': 534,\n",
              " 'girls': 535,\n",
              " 'viewer': 536,\n",
              " 'starts': 537,\n",
              " 'city': 538,\n",
              " 'myself': 539,\n",
              " 'decent': 540,\n",
              " 'highly': 541,\n",
              " 'stop': 542,\n",
              " 'type': 543,\n",
              " 'self': 544,\n",
              " 'god': 545,\n",
              " 'says': 546,\n",
              " 'group': 547,\n",
              " 'anyway': 548,\n",
              " 'voice': 549,\n",
              " 'took': 550,\n",
              " 'known': 551,\n",
              " 'blood': 552,\n",
              " 'kid': 553,\n",
              " 'heard': 554,\n",
              " 'happens': 555,\n",
              " 'except': 556,\n",
              " 'fight': 557,\n",
              " 'feeling': 558,\n",
              " 'experience': 559,\n",
              " 'coming': 560,\n",
              " 'slow': 561,\n",
              " 'daughter': 562,\n",
              " 'writer': 563,\n",
              " 'stories': 564,\n",
              " 'moment': 565,\n",
              " 'leave': 566,\n",
              " 'told': 567,\n",
              " 'extremely': 568,\n",
              " 'score': 569,\n",
              " 'violence': 570,\n",
              " 'involved': 571,\n",
              " 'police': 572,\n",
              " 'strong': 573,\n",
              " 'chance': 574,\n",
              " 'lack': 575,\n",
              " 'cannot': 576,\n",
              " 'hit': 577,\n",
              " 'roles': 578,\n",
              " 'hilarious': 579,\n",
              " 's': 580,\n",
              " 'happen': 581,\n",
              " 'wonder': 582,\n",
              " 'particularly': 583,\n",
              " 'ok': 584,\n",
              " 'including': 585,\n",
              " 'living': 586,\n",
              " 'save': 587,\n",
              " 'looked': 588,\n",
              " \"wouldn't\": 589,\n",
              " 'crap': 590,\n",
              " 'simple': 591,\n",
              " 'please': 592,\n",
              " 'murder': 593,\n",
              " 'cool': 594,\n",
              " 'obvious': 595,\n",
              " 'happened': 596,\n",
              " 'complete': 597,\n",
              " 'cut': 598,\n",
              " 'serious': 599,\n",
              " 'age': 600,\n",
              " 'gore': 601,\n",
              " 'attempt': 602,\n",
              " 'hell': 603,\n",
              " 'ago': 604,\n",
              " 'song': 605,\n",
              " 'shown': 606,\n",
              " 'taken': 607,\n",
              " 'english': 608,\n",
              " 'james': 609,\n",
              " 'robert': 610,\n",
              " 'david': 611,\n",
              " 'seriously': 612,\n",
              " 'released': 613,\n",
              " 'reality': 614,\n",
              " 'opening': 615,\n",
              " 'interest': 616,\n",
              " 'jokes': 617,\n",
              " 'across': 618,\n",
              " 'none': 619,\n",
              " 'hero': 620,\n",
              " 'today': 621,\n",
              " 'exactly': 622,\n",
              " 'possible': 623,\n",
              " 'alone': 624,\n",
              " 'sad': 625,\n",
              " 'brother': 626,\n",
              " 'number': 627,\n",
              " 'career': 628,\n",
              " 'saying': 629,\n",
              " \"film's\": 630,\n",
              " 'usually': 631,\n",
              " 'hours': 632,\n",
              " 'cinematography': 633,\n",
              " 'talent': 634,\n",
              " 'view': 635,\n",
              " 'running': 636,\n",
              " 'yourself': 637,\n",
              " 'annoying': 638,\n",
              " 'relationship': 639,\n",
              " 'documentary': 640,\n",
              " 'wish': 641,\n",
              " 'order': 642,\n",
              " 'huge': 643,\n",
              " 'shots': 644,\n",
              " 'whose': 645,\n",
              " 'ridiculous': 646,\n",
              " 'taking': 647,\n",
              " 'important': 648,\n",
              " 'light': 649,\n",
              " 'body': 650,\n",
              " 'middle': 651,\n",
              " 'level': 652,\n",
              " 'ends': 653,\n",
              " 'started': 654,\n",
              " 'call': 655,\n",
              " 'female': 656,\n",
              " \"i'll\": 657,\n",
              " 'husband': 658,\n",
              " 'four': 659,\n",
              " 'power': 660,\n",
              " 'word': 661,\n",
              " 'turned': 662,\n",
              " 'major': 663,\n",
              " 'opinion': 664,\n",
              " 'change': 665,\n",
              " 'mostly': 666,\n",
              " 'usual': 667,\n",
              " 'scary': 668,\n",
              " 'silly': 669,\n",
              " 'rating': 670,\n",
              " 'beyond': 671,\n",
              " 'somewhat': 672,\n",
              " 'happy': 673,\n",
              " 'ones': 674,\n",
              " 'words': 675,\n",
              " 'room': 676,\n",
              " 'knew': 677,\n",
              " 'knows': 678,\n",
              " 'country': 679,\n",
              " 'disappointed': 680,\n",
              " 'talking': 681,\n",
              " 'novel': 682,\n",
              " 'apparently': 683,\n",
              " 'non': 684,\n",
              " 'strange': 685,\n",
              " 'upon': 686,\n",
              " 'attention': 687,\n",
              " 'single': 688,\n",
              " 'basically': 689,\n",
              " 'finds': 690,\n",
              " 'cheap': 691,\n",
              " 'modern': 692,\n",
              " 'due': 693,\n",
              " 'jack': 694,\n",
              " 'musical': 695,\n",
              " 'television': 696,\n",
              " 'problems': 697,\n",
              " 'miss': 698,\n",
              " 'episodes': 699,\n",
              " 'clearly': 700,\n",
              " 'local': 701,\n",
              " '7': 702,\n",
              " 'british': 703,\n",
              " 'thriller': 704,\n",
              " 'talk': 705,\n",
              " 'events': 706,\n",
              " 'five': 707,\n",
              " 'sequence': 708,\n",
              " \"aren't\": 709,\n",
              " 'class': 710,\n",
              " 'french': 711,\n",
              " 'moving': 712,\n",
              " 'ten': 713,\n",
              " 'fast': 714,\n",
              " 'earth': 715,\n",
              " 'review': 716,\n",
              " 'tells': 717,\n",
              " 'predictable': 718,\n",
              " 'songs': 719,\n",
              " 'team': 720,\n",
              " 'comic': 721,\n",
              " 'straight': 722,\n",
              " '8': 723,\n",
              " 'whether': 724,\n",
              " 'die': 725,\n",
              " 'add': 726,\n",
              " 'dialog': 727,\n",
              " 'entertainment': 728,\n",
              " 'above': 729,\n",
              " 'sets': 730,\n",
              " 'future': 731,\n",
              " 'enjoyable': 732,\n",
              " 'appears': 733,\n",
              " 'near': 734,\n",
              " 'space': 735,\n",
              " 'easily': 736,\n",
              " 'hate': 737,\n",
              " 'soundtrack': 738,\n",
              " 'bring': 739,\n",
              " 'giving': 740,\n",
              " 'lots': 741,\n",
              " 'similar': 742,\n",
              " 'romantic': 743,\n",
              " 'george': 744,\n",
              " 'supporting': 745,\n",
              " 'release': 746,\n",
              " 'mention': 747,\n",
              " 'filmed': 748,\n",
              " 'within': 749,\n",
              " 'message': 750,\n",
              " 'sequel': 751,\n",
              " 'clear': 752,\n",
              " 'falls': 753,\n",
              " \"haven't\": 754,\n",
              " 'needs': 755,\n",
              " 'dull': 756,\n",
              " 'suspense': 757,\n",
              " 'eye': 758,\n",
              " 'bunch': 759,\n",
              " 'surprised': 760,\n",
              " 'showing': 761,\n",
              " 'sorry': 762,\n",
              " 'tried': 763,\n",
              " 'certain': 764,\n",
              " 'working': 765,\n",
              " 'easy': 766,\n",
              " 'ways': 767,\n",
              " 'theme': 768,\n",
              " 'theater': 769,\n",
              " 'among': 770,\n",
              " 'named': 771,\n",
              " \"what's\": 772,\n",
              " 'storyline': 773,\n",
              " 'monster': 774,\n",
              " 'king': 775,\n",
              " 'stay': 776,\n",
              " 'effort': 777,\n",
              " 'fall': 778,\n",
              " 'stand': 779,\n",
              " 'minute': 780,\n",
              " 'gone': 781,\n",
              " 'rock': 782,\n",
              " 'using': 783,\n",
              " '9': 784,\n",
              " 'feature': 785,\n",
              " 'buy': 786,\n",
              " 'comments': 787,\n",
              " \"'\": 788,\n",
              " 'typical': 789,\n",
              " 't': 790,\n",
              " 'sister': 791,\n",
              " 'editing': 792,\n",
              " 'tale': 793,\n",
              " 'avoid': 794,\n",
              " 'mystery': 795,\n",
              " 'dr': 796,\n",
              " 'deal': 797,\n",
              " 'doubt': 798,\n",
              " 'fantastic': 799,\n",
              " 'kept': 800,\n",
              " 'nearly': 801,\n",
              " 'feels': 802,\n",
              " 'subject': 803,\n",
              " 'okay': 804,\n",
              " 'viewing': 805,\n",
              " 'elements': 806,\n",
              " 'oscar': 807,\n",
              " 'check': 808,\n",
              " 'realistic': 809,\n",
              " 'points': 810,\n",
              " 'means': 811,\n",
              " 'greatest': 812,\n",
              " 'herself': 813,\n",
              " 'parents': 814,\n",
              " 'famous': 815,\n",
              " 'imagine': 816,\n",
              " 'rent': 817,\n",
              " 'viewers': 818,\n",
              " 'richard': 819,\n",
              " 'crime': 820,\n",
              " 'form': 821,\n",
              " 'peter': 822,\n",
              " 'actual': 823,\n",
              " 'lady': 824,\n",
              " 'general': 825,\n",
              " 'dog': 826,\n",
              " 'follow': 827,\n",
              " 'believable': 828,\n",
              " 'period': 829,\n",
              " 'red': 830,\n",
              " 'brought': 831,\n",
              " 'move': 832,\n",
              " 'material': 833,\n",
              " 'forget': 834,\n",
              " 'somehow': 835,\n",
              " 'begins': 836,\n",
              " 're': 837,\n",
              " 'reviews': 838,\n",
              " 'animation': 839,\n",
              " 'paul': 840,\n",
              " \"you've\": 841,\n",
              " 'leads': 842,\n",
              " 'weak': 843,\n",
              " 'figure': 844,\n",
              " 'surprise': 845,\n",
              " 'sit': 846,\n",
              " 'hear': 847,\n",
              " 'average': 848,\n",
              " 'open': 849,\n",
              " 'sequences': 850,\n",
              " 'killing': 851,\n",
              " 'atmosphere': 852,\n",
              " 'eventually': 853,\n",
              " 'learn': 854,\n",
              " 'tom': 855,\n",
              " 'premise': 856,\n",
              " '20': 857,\n",
              " 'wait': 858,\n",
              " 'sci': 859,\n",
              " 'deep': 860,\n",
              " 'fi': 861,\n",
              " 'expected': 862,\n",
              " 'whatever': 863,\n",
              " 'indeed': 864,\n",
              " 'particular': 865,\n",
              " 'poorly': 866,\n",
              " 'note': 867,\n",
              " 'lame': 868,\n",
              " 'imdb': 869,\n",
              " 'dance': 870,\n",
              " 'situation': 871,\n",
              " 'shame': 872,\n",
              " 'third': 873,\n",
              " 'box': 874,\n",
              " 'york': 875,\n",
              " 'truth': 876,\n",
              " 'decided': 877,\n",
              " 'free': 878,\n",
              " 'hot': 879,\n",
              " \"who's\": 880,\n",
              " 'difficult': 881,\n",
              " 'season': 882,\n",
              " 'needed': 883,\n",
              " 'acted': 884,\n",
              " 'leaves': 885,\n",
              " 'unless': 886,\n",
              " 'romance': 887,\n",
              " 'possibly': 888,\n",
              " 'emotional': 889,\n",
              " 'sexual': 890,\n",
              " 'gay': 891,\n",
              " 'boys': 892,\n",
              " 'footage': 893,\n",
              " 'write': 894,\n",
              " 'western': 895,\n",
              " 'credits': 896,\n",
              " 'forced': 897,\n",
              " 'became': 898,\n",
              " 'memorable': 899,\n",
              " 'reading': 900,\n",
              " 'doctor': 901,\n",
              " 'otherwise': 902,\n",
              " 'begin': 903,\n",
              " 'de': 904,\n",
              " 'air': 905,\n",
              " 'crew': 906,\n",
              " 'question': 907,\n",
              " 'society': 908,\n",
              " 'meet': 909,\n",
              " 'male': 910,\n",
              " 'meets': 911,\n",
              " \"let's\": 912,\n",
              " 'plus': 913,\n",
              " 'cheesy': 914,\n",
              " 'hands': 915,\n",
              " 'superb': 916,\n",
              " 'screenplay': 917,\n",
              " 'beauty': 918,\n",
              " 'interested': 919,\n",
              " 'features': 920,\n",
              " 'street': 921,\n",
              " 'perfectly': 922,\n",
              " 'masterpiece': 923,\n",
              " 'whom': 924,\n",
              " 'laughs': 925,\n",
              " 'nature': 926,\n",
              " 'stage': 927,\n",
              " 'effect': 928,\n",
              " 'forward': 929,\n",
              " 'comment': 930,\n",
              " 'nor': 931,\n",
              " 'sounds': 932,\n",
              " 'e': 933,\n",
              " 'previous': 934,\n",
              " 'badly': 935,\n",
              " 'japanese': 936,\n",
              " 'weird': 937,\n",
              " 'island': 938,\n",
              " 'inside': 939,\n",
              " 'personal': 940,\n",
              " 'quickly': 941,\n",
              " 'total': 942,\n",
              " 'keeps': 943,\n",
              " 'towards': 944,\n",
              " 'result': 945,\n",
              " 'america': 946,\n",
              " 'crazy': 947,\n",
              " 'battle': 948,\n",
              " 'worked': 949,\n",
              " 'setting': 950,\n",
              " 'incredibly': 951,\n",
              " 'background': 952,\n",
              " 'earlier': 953,\n",
              " 'mess': 954,\n",
              " 'cop': 955,\n",
              " 'writers': 956,\n",
              " 'fire': 957,\n",
              " 'copy': 958,\n",
              " 'realize': 959,\n",
              " 'unique': 960,\n",
              " 'dumb': 961,\n",
              " 'powerful': 962,\n",
              " 'mark': 963,\n",
              " 'lee': 964,\n",
              " 'business': 965,\n",
              " 'rate': 966,\n",
              " 'dramatic': 967,\n",
              " 'older': 968,\n",
              " 'pay': 969,\n",
              " 'following': 970,\n",
              " 'directors': 971,\n",
              " 'joke': 972,\n",
              " 'girlfriend': 973,\n",
              " 'plenty': 974,\n",
              " 'directing': 975,\n",
              " 'various': 976,\n",
              " 'creepy': 977,\n",
              " 'baby': 978,\n",
              " 'appear': 979,\n",
              " 'development': 980,\n",
              " 'brings': 981,\n",
              " 'front': 982,\n",
              " 'dream': 983,\n",
              " 'ask': 984,\n",
              " 'water': 985,\n",
              " 'rich': 986,\n",
              " 'bill': 987,\n",
              " 'admit': 988,\n",
              " 'apart': 989,\n",
              " 'joe': 990,\n",
              " 'political': 991,\n",
              " 'fairly': 992,\n",
              " 'leading': 993,\n",
              " 'reasons': 994,\n",
              " 'spent': 995,\n",
              " 'portrayed': 996,\n",
              " 'telling': 997,\n",
              " 'cover': 998,\n",
              " 'outside': 999,\n",
              " 'fighting': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZGGeitPt2WT"
      },
      "source": [
        "Tokenizer로 훈련 데이터의 텍스트를 정수로 변환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sOOcMmI3t2WT"
      },
      "outputs": [],
      "source": [
        "x_train_tokens = tokenizer.texts_to_sequences(x_train_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSI5KoKwt2WT"
      },
      "source": [
        "For example, here is a text from the training-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "bD7v-6HVt2WU",
        "outputId": "53b30816-4052-4bc0-8e25-d4d8de69bb28"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hmmmm, want a little romance with your mystery? This has it. I think if the romance was ditched this would have made for a better movie. But how could the romance be ditched when the story's borrowed from something called a Harlequin Romance novel, whatever the heck that is. Had the romance been ditched, the story might have been a little too weak. The mystery here wasn't too bad, quite interesting but nothing on the level of Mission Impossible international espionage. Oh well. I thought Mel Harris was pretty good; her short skirts, i think, added some sex appeal... but this Rob Stewart guy probably could have been better cast, maybe with a more well known TV movie actor. The directing was decent and the writing could have been improved on - both could have been a little edgier, a little darker, more adventurous. One thing that was great about this was the use of real European locations. That could easily have been changed so this could have been filmed in Canada but they really were in magnificently beautiful places like Budapest. Possibly a drawback was the director and/or cinematographer's choice to frame certain shots picture postcard perfect. Not good. Had this been a more dramatic motion picture shot for the big screen, picture postcard perfect scenes really need to take a backseat and just be a nice part of the background. This was just a tv-movie, though, so they had to add some Ummmph to the picture and some of that Ummmph came from the scenery. Overall, twasn't really a bad movie. I'll tell you what, this was absolutely the best Canadian-Hungarian production I have ever seen! (and the only that i know of.) I hereby proclaim this to be a mediocre made-for-tv movie, giving it a grade of C-\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# TODO: 훈련 데이터의 0번째 텍스트만 출력해보기\n",
        "x_train_text[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX8tA1JQt2WU"
      },
      "source": [
        "This text corresponds to the following list of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVyrza1ct2WU",
        "outputId": "f78d8a25-ae51-4326-c496-ecfc020b336d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 178,    3,  120,  887,   16,  125,  795,   11,   45,    9,   10,\n",
              "        101,   43,    1,  887,   13,   11,   58,   25,   90,   15,    3,\n",
              "        126,   17,   18,   85,   98,    1,  887,   26,   50,    1, 6768,\n",
              "       4352,   36,  137,  433,    3,  887,  682,  863,    1, 2150,   12,\n",
              "          6,   66,    1,  887,   75,    1,   64,  233,   25,   75,    3,\n",
              "        120,   96,  843,    1,  795,  133,  292,   96,   74,  179,  218,\n",
              "         18,  161,   20,    1,  652,    4, 1877, 1189, 2015, 7833,  429,\n",
              "         69,   10,  190, 3425, 2446,   13,  180,   49,   40,  334,   10,\n",
              "        101, 1248,   47,  376, 1324,   18,   11, 2249, 1582,  219,  239,\n",
              "         98,   25,   75,  126,  174,  273,   16,    3,   51,   69,  551,\n",
              "        240,   17,  291,    1,  975,   13,  540,    2,    1,  490,   98,\n",
              "         25,   75, 3889,   20,  195,   98,   25,   75,    3,  120,    3,\n",
              "        120, 4103,   51, 8602,   27,  148,   12,   13,   78,   42,   11,\n",
              "         13,    1,  356,    4,  144, 1955, 1816,   12,   98,  736,   25,\n",
              "         75, 1187,   34,   11,   98,   25,   75,  748,    8, 3847,   18,\n",
              "         33,   62,   70,    8, 9408,  311, 1363,   37,  888,    3,   13,\n",
              "          1,  164,    2,   38, 1111,    5, 1956,  764,  644,  435,  399,\n",
              "         21,   49,   66,   11,   75,    3,   51,  967, 1293,  435,  319,\n",
              "         15,    1,  191,  258,  435,  399,  134,   62,  362,    5,  187,\n",
              "          3,    2,   39,   26,    3,  331,  173,    4,    1,  952,   11,\n",
              "         13,   39,    3,  240,   17,  151,   34,   33,   66,    5,  726,\n",
              "         47,    5,    1,  435,    2,   47,    4,   12,  384,   36,    1,\n",
              "       1305,  437,   62,    3,   74,   17,  657,  380,   22,   48,   11,\n",
              "         13,  419,    1,  116, 2367, 8089,  353,   10,   25,  123,  107,\n",
              "          2,    1,   61,   12,   10,  118,    4,   10,   11,    5,   26,\n",
              "          3, 1564,   90,   15,  240,   17,  740,    9,    3, 1235,    4,\n",
              "       1124])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# TODO: 정수로 변환된 0번째 텍스트만 출력해보기\n",
        "# the는 1로, and는 2로, a는 3으로, of는 4로 변환됐는지 살펴보세요.\n",
        "np.array(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvOL1jHAt2WU"
      },
      "source": [
        "We also need to convert the texts in the test-set to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FAjPUecQt2WU"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 테스트 데이터의 텍스트를 정수로 변환해보세요.\n",
        "x_test_tokens = tokenizer.texts_to_sequences(x_test_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHGyZPoEt2WU"
      },
      "source": [
        "## Padding and Truncating Data\n",
        "\n",
        "순환신경망은 임의의 길이를 지닌 입력을 처리할 수 있지만 훈련의 속도를 높이기 위해 동일 길이를 지닌 입력 데이터의 묶음 (batch) 단위로 훈련이 이루어지는 관계로 편의상 모든 입력 데이터의 길이를 동일하게 합니다. 모든 입력 데이터의 길이를 동일하게 하는 방법은 크게 두 가지가 있습니다.\n",
        "\n",
        "1. 가장 길이가 긴 리뷰의 길이에 맞추어 다른 리뷰들의 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "2. 적당한 길이를 정한다음 길이가 긴 리뷰는 앞, 혹은 뒤를 잘라내고 짧은 리뷰는 앞, 혹은 뒤에 공백을 삽입한다.\n",
        "\n",
        "첫 번째 방법은 지나치게 긴 리뷰가 있을 경우 다른 리뷰들에 지나치게 많은 공백이 들어가 데이터셋이 커져 훈련하는데 오래 걸리고 메모리의 낭비도 심해지는 단점이 있어 우리는 두 번째 방법을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "RlDFg_w0t2WU"
      },
      "outputs": [],
      "source": [
        "# 각 리뷰에 단어 (토큰)가 몇 개씩 들어있는지 세어봅시다.\n",
        "\n",
        "num_tokens = [] # 빈 리스트 생성\n",
        "\n",
        "# TODO: num_tokens에 각 리뷰의 단어 숫자를 저장하세요.\n",
        "# num_tokens 원소는 50,000개이며 처음 25,000은 훈련 데이터의 단어숫자가 저장되어 있고,\n",
        "# 나머지 25,000은 테스트 데이터의 단어숫자가 저장되어 있습니다.\n",
        "# Hint: x_train_tokens[0]은 첫 번째 훈련 데이터의 단어들이 저장되어 있습니다. 그 길이는 len 함수로 알 수 있습니다.\n",
        "for tokens in x_train_tokens:\n",
        "    num_tokens.append(len(tokens))\n",
        "\n",
        "for tokens in x_test_tokens:\n",
        "    num_tokens.append(len(tokens))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9kwihq8yLf6H"
      },
      "outputs": [],
      "source": [
        "# TODO: num_tokens 리스트를 num_tokens NumPy 배열로 변환하세요.\n",
        "num_tokens = np.array(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYdxLK99t2WU"
      },
      "source": [
        "The average number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0kuTohct2WV",
        "outputId": "f9c7ade7-1228-4f1d-ece2-8bc49eea4789"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221.27716"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# 모든 리뷰에서 사용한 평균 단어의 수는 다음과 같습니다.\n",
        "np.mean(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDu36sCDt2WV"
      },
      "source": [
        "The maximum number of tokens in a sequence is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKujyc3It2WV",
        "outputId": "857ad992-be9a-45c0-a2a9-1649b2f1efb2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2209"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# TODO: 가장 긴 리뷰의 단어수를 출력해보세요.\n",
        "# Hint: np.max\n",
        "np.max(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "BzLYeiVRBXc2",
        "outputId": "6cd63534-cf6f-4a6b-e7d8-3dfe4e13eb5d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# 리뷰에 사용된 단어수(토큰수)에 따른 리뷰의 수 살펴보기\n",
        "# 대부분의 리뷰가 500개 이하의 토큰만 사용했음을 알 수 있음\n",
        "plt.hist(num_tokens, bins=50)\n",
        "plt.xlabel('Number of tokens')\n",
        "plt.ylabel('Number of reviews')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwAj_aOIt2WV"
      },
      "source": [
        "The max number of tokens we will allow is set to the average plus 2 standard deviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M8wDkUJt2WV",
        "outputId": "2c13d20d-f2fa-463c-f78a-5332eba74a63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# TODO: num_tokens에서 평균 + 2 * 표준편차를 구하여 우리가 허용할 최대 단어의 개수인 max_tokens를 설정해보세요.\n",
        "# num_tokens가 정규분포를 따른다면 97% 이상이 max_tokens보다 적은 단어만을 사용할 것입니다.\n",
        "# Hint: np.mean과 np.std 사용\n",
        "\n",
        "max_tokens = np.mean(num_tokens) + 2*np.std(num_tokens)\n",
        "max_tokens = int(max_tokens)\n",
        "max_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B34-DJ_ot2WV"
      },
      "source": [
        "This covers about 95% of the data-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg-lkIWjt2WV",
        "outputId": "6e4ee073-32f0-4886-c91d-f770dd0ff2b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.94534"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# max_tokens보다 짧은 리뷰의 비율을 출력하는 코드\n",
        "np.sum(num_tokens < max_tokens) / len(num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX2jx6eINEJ_"
      },
      "source": [
        "리뷰의 94% 이상은 max_tokens보다 짧으므로 공백을 삽입하여 길이를 max_tokens로 맞춰주고, max_tokens보다 긴 리뷰의 길이는 max_tokens로 자를 필요가 있습니다. 모든 리뷰를 가장 길었던 리뷰의 길이로 맞췄다면 데이터셋 크기가 굉장히 커졌을 것입니다.<br>\n",
        "\n",
        "공백을 삽입할 때 리뷰의 앞에 ('pre') 삽입할 수도 있고 마지막 ('post')에 삽입할 수도 있는데 인공신경망에 혼란을 주지 않기 위해 일관된 방식으로 삽입해야 합니다. (자르기도 동일)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "M8Y_JY2St2WW"
      },
      "outputs": [],
      "source": [
        "# 공백을 리뷰의 앞에 삽입\n",
        "pad = 'pre'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XkY-yQDgt2WW"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터의 길이를 max_tokens로 맞춰주기\n",
        "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens,  # 입력의 최대 길이는 max_tokens\n",
        "                            padding=pad, truncating=pad)        # 패딩과 자르기 모두 리뷰의 앞부분에서 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "okwoNpgGt2WW"
      },
      "outputs": [],
      "source": [
        "# TODO: 테스트 데이터의 길이를 max_tokens로 맞춰주세요.\n",
        "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens,  # 입력의 최대 길이는 max_tokens\n",
        "                            padding=pad, truncating=pad)        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCKTP6vPt2WW"
      },
      "source": [
        "We have now transformed the training-set into one big matrix of integers (tokens) with this shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVp4YkW7t2WW",
        "outputId": "5579ec32-0a0a-4323-82cb-6d42381b67a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# 훈련 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_train_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrnD4H3Zt2WW"
      },
      "source": [
        "The matrix for the test-set has the same shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJRJa6rgt2WW",
        "scrolled": true,
        "outputId": "b708188b-cc10-4b1d-c177-d0eddffa60eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# 테스트 데이터: (리뷰 개수, 각 리뷰의 길이)\n",
        "x_test_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxij8nRt2WW"
      },
      "source": [
        "For example, we had the following sequence of tokens above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjIbxctIt2WX",
        "outputId": "65c1fec2-bec4-4b59-f895-a63c2c24b54c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 178,    3,  120,  887,   16,  125,  795,   11,   45,    9,   10,\n",
              "        101,   43,    1,  887,   13,   11,   58,   25,   90,   15,    3,\n",
              "        126,   17,   18,   85,   98,    1,  887,   26,   50,    1, 6768,\n",
              "       4352,   36,  137,  433,    3,  887,  682,  863,    1, 2150,   12,\n",
              "          6,   66,    1,  887,   75,    1,   64,  233,   25,   75,    3,\n",
              "        120,   96,  843,    1,  795,  133,  292,   96,   74,  179,  218,\n",
              "         18,  161,   20,    1,  652,    4, 1877, 1189, 2015, 7833,  429,\n",
              "         69,   10,  190, 3425, 2446,   13,  180,   49,   40,  334,   10,\n",
              "        101, 1248,   47,  376, 1324,   18,   11, 2249, 1582,  219,  239,\n",
              "         98,   25,   75,  126,  174,  273,   16,    3,   51,   69,  551,\n",
              "        240,   17,  291,    1,  975,   13,  540,    2,    1,  490,   98,\n",
              "         25,   75, 3889,   20,  195,   98,   25,   75,    3,  120,    3,\n",
              "        120, 4103,   51, 8602,   27,  148,   12,   13,   78,   42,   11,\n",
              "         13,    1,  356,    4,  144, 1955, 1816,   12,   98,  736,   25,\n",
              "         75, 1187,   34,   11,   98,   25,   75,  748,    8, 3847,   18,\n",
              "         33,   62,   70,    8, 9408,  311, 1363,   37,  888,    3,   13,\n",
              "          1,  164,    2,   38, 1111,    5, 1956,  764,  644,  435,  399,\n",
              "         21,   49,   66,   11,   75,    3,   51,  967, 1293,  435,  319,\n",
              "         15,    1,  191,  258,  435,  399,  134,   62,  362,    5,  187,\n",
              "          3,    2,   39,   26,    3,  331,  173,    4,    1,  952,   11,\n",
              "         13,   39,    3,  240,   17,  151,   34,   33,   66,    5,  726,\n",
              "         47,    5,    1,  435,    2,   47,    4,   12,  384,   36,    1,\n",
              "       1305,  437,   62,    3,   74,   17,  657,  380,   22,   48,   11,\n",
              "         13,  419,    1,  116, 2367, 8089,  353,   10,   25,  123,  107,\n",
              "          2,    1,   61,   12,   10,  118,    4,   10,   11,    5,   26,\n",
              "          3, 1564,   90,   15,  240,   17,  740,    9,    3, 1235,    4,\n",
              "       1124])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# 패딩 전의 0번째 훈련 데이터 \n",
        "np.array(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVbARkK0t2WX"
      },
      "source": [
        "This has simply been padded to create the following sequence. Note that when this is input to the Recurrent Neural Network, then it first inputs a lot of zeros. If we had padded 'post' then it would input the integer-tokens first and then a lot of zeros. This may confuse the Recurrent Neural Network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvblmFmht2WX",
        "outputId": "b83963a2-bf90-4a62-ff7f-41ff7b8a1193"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,  178,    3,  120,  887,   16,  125,  795,\n",
              "         11,   45,    9,   10,  101,   43,    1,  887,   13,   11,   58,\n",
              "         25,   90,   15,    3,  126,   17,   18,   85,   98,    1,  887,\n",
              "         26,   50,    1, 6768, 4352,   36,  137,  433,    3,  887,  682,\n",
              "        863,    1, 2150,   12,    6,   66,    1,  887,   75,    1,   64,\n",
              "        233,   25,   75,    3,  120,   96,  843,    1,  795,  133,  292,\n",
              "         96,   74,  179,  218,   18,  161,   20,    1,  652,    4, 1877,\n",
              "       1189, 2015, 7833,  429,   69,   10,  190, 3425, 2446,   13,  180,\n",
              "         49,   40,  334,   10,  101, 1248,   47,  376, 1324,   18,   11,\n",
              "       2249, 1582,  219,  239,   98,   25,   75,  126,  174,  273,   16,\n",
              "          3,   51,   69,  551,  240,   17,  291,    1,  975,   13,  540,\n",
              "          2,    1,  490,   98,   25,   75, 3889,   20,  195,   98,   25,\n",
              "         75,    3,  120,    3,  120, 4103,   51, 8602,   27,  148,   12,\n",
              "         13,   78,   42,   11,   13,    1,  356,    4,  144, 1955, 1816,\n",
              "         12,   98,  736,   25,   75, 1187,   34,   11,   98,   25,   75,\n",
              "        748,    8, 3847,   18,   33,   62,   70,    8, 9408,  311, 1363,\n",
              "         37,  888,    3,   13,    1,  164,    2,   38, 1111,    5, 1956,\n",
              "        764,  644,  435,  399,   21,   49,   66,   11,   75,    3,   51,\n",
              "        967, 1293,  435,  319,   15,    1,  191,  258,  435,  399,  134,\n",
              "         62,  362,    5,  187,    3,    2,   39,   26,    3,  331,  173,\n",
              "          4,    1,  952,   11,   13,   39,    3,  240,   17,  151,   34,\n",
              "         33,   66,    5,  726,   47,    5,    1,  435,    2,   47,    4,\n",
              "         12,  384,   36,    1, 1305,  437,   62,    3,   74,   17,  657,\n",
              "        380,   22,   48,   11,   13,  419,    1,  116, 2367, 8089,  353,\n",
              "         10,   25,  123,  107,    2,    1,   61,   12,   10,  118,    4,\n",
              "         10,   11,    5,   26,    3, 1564,   90,   15,  240,   17,  740,\n",
              "          9,    3, 1235,    4, 1124], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# TODO: 패딩 후의 0번째 훈련 데이터를 출력해보세요.\n",
        "np.array(x_train_pad[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuEuwTb7t2WX"
      },
      "source": [
        "## Tokenizer Inverse Map\n",
        "\n",
        "정수의 배열을 다시 텍스트로 바꿔주는 함수를 만들어봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GcQFzRXkt2WX"
      },
      "outputs": [],
      "source": [
        "idx = tokenizer.word_index # tokenizer는 단어 (key)-> 정수 (value)이므로 \n",
        "inverse_map = dict(zip(idx.values(), idx.keys())) # 정수 (value) -> 단어 (key)로 보내면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUsOFMx3t2WX"
      },
      "source": [
        "Helper-function for converting a list of tokens back to a string of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CmNXSZX3t2WX"
      },
      "outputs": [],
      "source": [
        "# tokens_to_string 함수에 정수 배열을 입력하면 텍스트로 출력됩니다.\n",
        "def tokens_to_string(tokens):\n",
        "    # Map from tokens back to words.\n",
        "    words = [inverse_map[token] for token in tokens if token != 0]\n",
        "    \n",
        "    # Concatenate all words.\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8joKACPJt2WX"
      },
      "source": [
        "For example, this is the original text from the data-set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "G7-Rex8ot2WX",
        "scrolled": true,
        "outputId": "4ea00fc9-9852-4283-f6a0-98e0ea461cec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hmmmm, want a little romance with your mystery? This has it. I think if the romance was ditched this would have made for a better movie. But how could the romance be ditched when the story's borrowed from something called a Harlequin Romance novel, whatever the heck that is. Had the romance been ditched, the story might have been a little too weak. The mystery here wasn't too bad, quite interesting but nothing on the level of Mission Impossible international espionage. Oh well. I thought Mel Harris was pretty good; her short skirts, i think, added some sex appeal... but this Rob Stewart guy probably could have been better cast, maybe with a more well known TV movie actor. The directing was decent and the writing could have been improved on - both could have been a little edgier, a little darker, more adventurous. One thing that was great about this was the use of real European locations. That could easily have been changed so this could have been filmed in Canada but they really were in magnificently beautiful places like Budapest. Possibly a drawback was the director and/or cinematographer's choice to frame certain shots picture postcard perfect. Not good. Had this been a more dramatic motion picture shot for the big screen, picture postcard perfect scenes really need to take a backseat and just be a nice part of the background. This was just a tv-movie, though, so they had to add some Ummmph to the picture and some of that Ummmph came from the scenery. Overall, twasn't really a bad movie. I'll tell you what, this was absolutely the best Canadian-Hungarian production I have ever seen! (and the only that i know of.) I hereby proclaim this to be a mediocre made-for-tv movie, giving it a grade of C-\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# 훈련 데이터의 0번째 리뷰 실제 텍스트\n",
        "x_train_text[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1q-SfSwdQQS2",
        "outputId": "011298f5-5392-4939-a17d-a67d52d0da9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 178,    3,  120,  887,   16,  125,  795,   11,   45,    9,   10,\n",
              "        101,   43,    1,  887,   13,   11,   58,   25,   90,   15,    3,\n",
              "        126,   17,   18,   85,   98,    1,  887,   26,   50,    1, 6768,\n",
              "       4352,   36,  137,  433,    3,  887,  682,  863,    1, 2150,   12,\n",
              "          6,   66,    1,  887,   75,    1,   64,  233,   25,   75,    3,\n",
              "        120,   96,  843,    1,  795,  133,  292,   96,   74,  179,  218,\n",
              "         18,  161,   20,    1,  652,    4, 1877, 1189, 2015, 7833,  429,\n",
              "         69,   10,  190, 3425, 2446,   13,  180,   49,   40,  334,   10,\n",
              "        101, 1248,   47,  376, 1324,   18,   11, 2249, 1582,  219,  239,\n",
              "         98,   25,   75,  126,  174,  273,   16,    3,   51,   69,  551,\n",
              "        240,   17,  291,    1,  975,   13,  540,    2,    1,  490,   98,\n",
              "         25,   75, 3889,   20,  195,   98,   25,   75,    3,  120,    3,\n",
              "        120, 4103,   51, 8602,   27,  148,   12,   13,   78,   42,   11,\n",
              "         13,    1,  356,    4,  144, 1955, 1816,   12,   98,  736,   25,\n",
              "         75, 1187,   34,   11,   98,   25,   75,  748,    8, 3847,   18,\n",
              "         33,   62,   70,    8, 9408,  311, 1363,   37,  888,    3,   13,\n",
              "          1,  164,    2,   38, 1111,    5, 1956,  764,  644,  435,  399,\n",
              "         21,   49,   66,   11,   75,    3,   51,  967, 1293,  435,  319,\n",
              "         15,    1,  191,  258,  435,  399,  134,   62,  362,    5,  187,\n",
              "          3,    2,   39,   26,    3,  331,  173,    4,    1,  952,   11,\n",
              "         13,   39,    3,  240,   17,  151,   34,   33,   66,    5,  726,\n",
              "         47,    5,    1,  435,    2,   47,    4,   12,  384,   36,    1,\n",
              "       1305,  437,   62,    3,   74,   17,  657,  380,   22,   48,   11,\n",
              "         13,  419,    1,  116, 2367, 8089,  353,   10,   25,  123,  107,\n",
              "          2,    1,   61,   12,   10,  118,    4,   10,   11,    5,   26,\n",
              "          3, 1564,   90,   15,  240,   17,  740,    9,    3, 1235,    4,\n",
              "       1124])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# 정수로 변환된 훈련 데이터의 0번째 리뷰\n",
        "np.array(x_train_tokens[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ya16yBQrt2WY"
      },
      "source": [
        "We can recreate this text except for punctuation and other symbols, by converting the list of tokens back to words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "DqDwOJ9Lt2WY",
        "outputId": "c4d5f8a6-e6c5-435f-ddb0-64325e18229a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"want a little romance with your mystery this has it i think if the romance was this would have made for a better movie but how could the romance be when the story's borrowed from something called a romance novel whatever the heck that is had the romance been the story might have been a little too weak the mystery here wasn't too bad quite interesting but nothing on the level of mission impossible international espionage oh well i thought mel harris was pretty good her short i think added some sex appeal but this rob stewart guy probably could have been better cast maybe with a more well known tv movie actor the directing was decent and the writing could have been improved on both could have been a little a little darker more adventurous one thing that was great about this was the use of real european locations that could easily have been changed so this could have been filmed in canada but they really were in magnificently beautiful places like possibly a was the director and or choice to frame certain shots picture perfect not good had this been a more dramatic motion picture shot for the big screen picture perfect scenes really need to take a and just be a nice part of the background this was just a tv movie though so they had to add some to the picture and some of that came from the scenery overall really a bad movie i'll tell you what this was absolutely the best canadian hungarian production i have ever seen and the only that i know of i this to be a mediocre made for tv movie giving it a grade of c\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "# TODO: 정수 형태의 0번째 리뷰를 다시 단어로 변환해보세요.\n",
        "# 모두 소문자로 변경되고, 잘 쓰이지 않는 단어들은 삭제된 상태입니다.\n",
        "# Hint: 위에서 정의한 tokens_to_string 함수 이용\n",
        "tokens_to_string(np.array(x_train_tokens[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hamblZDft2WY"
      },
      "source": [
        "## Create the Recurrent Neural Network\n",
        "\n",
        "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8nObzn4ft2WY"
      },
      "outputs": [],
      "source": [
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggam4ai4t2WY"
      },
      "source": [
        "자연어 처리를 위한 순환 신경망의 가장 첫 번째 층은 임베딩층입니다. 우리가 선택한 10000개의 단어를 우리가 지정한 차원에 보내 비슷한 단어가 가까운 곳에 위치하도록 만들 것입니다. (Tokenizer가 만든 정수는 1: the, 2: and, 3: a, 4: of과 같이 뜻이 비슷하지도 않은데 단어들이 가까운 곳에 위치해 있습니다.)\n",
        "\n",
        "일반적으로 임베딩 차원은 100에서 300 정도를 사용하는데 일단 8차원만 사용해봅시다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ObHSK_FKt2WY"
      },
      "outputs": [],
      "source": [
        "embedding_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOGpWR8t2WY"
      },
      "source": [
        "The embedding-layer also needs to know the number of words in the vocabulary (`num_words`) and the length of the padded token-sequences (`max_tokens`). We also give this layer a name because we need to retrieve its weights further below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "sVBLbYdrt2WY"
      },
      "outputs": [],
      "source": [
        "# 임베딩층 추가\n",
        "model.add(Embedding(input_dim=num_words,      # 사용하는 단어의 개수\n",
        "                    output_dim=embedding_size,# 임베딩 차원\n",
        "                    input_length=max_tokens,  # 리뷰의 길이\n",
        "                    name='layer_embedding'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36FqWbR9t2WY"
      },
      "source": [
        "순환 유닛인 GRU를 추가하고 Dense로 묶어 하나의 출력에서 0과 1사이의 실수를 출력하도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "LnuPpp4zt2WY"
      },
      "outputs": [],
      "source": [
        "model.add(GRU(units=256))\n",
        "model.add(Dense(1, activation='sigmoid')) # 이전 층의 입력을 받아들여 0부터 1사이의 실수값 하나 출력 (sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUlN1Vxvt2WZ"
      },
      "source": [
        "Use the Adam optimizer with the given learning-rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "h9tJRHtZt2WZ"
      },
      "outputs": [],
      "source": [
        "optimizer = Adam(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kah4uUD3t2WZ"
      },
      "source": [
        "Compile the Keras model so it is ready for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "KRiczqCyt2WZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZINq_6Bkt2Wa",
        "outputId": "2422f395-d416-4cd8-bb2f-904876bde918"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " layer_embedding (Embedding)  (None, 544, 8)           80000     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 256)               204288    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 284,545\n",
            "Trainable params: 284,545\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYcJVRpt2Wa"
      },
      "source": [
        "## Train the Recurrent Neural Network\n",
        "\n",
        "We can now train the model. Note that we are using the data-set with the padded sequences. We use 5% of the training-set as a small validation-set, so we have a rough idea whether the model is generalizing well or if it is perhaps over-fitting to the training-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQxdKqGbt2Wa",
        "scrolled": true,
        "outputId": "a73a70c4-ee73-4dc7-b4c6-f28a4f071d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10000\n",
            "313/313 [==============================] - 69s 196ms/step - loss: 0.5759 - accuracy: 0.6988 - val_loss: 0.7491 - val_accuracy: 0.7278\n",
            "Epoch 2/10000\n",
            "313/313 [==============================] - 36s 116ms/step - loss: 0.3298 - accuracy: 0.8674 - val_loss: 0.3734 - val_accuracy: 0.8536\n",
            "Epoch 3/10000\n",
            "313/313 [==============================] - 28s 89ms/step - loss: 0.2351 - accuracy: 0.9083 - val_loss: 0.9021 - val_accuracy: 0.6878\n",
            "Epoch 4/10000\n",
            "313/313 [==============================] - 21s 68ms/step - loss: 0.1871 - accuracy: 0.9309 - val_loss: 0.3899 - val_accuracy: 0.8504\n",
            "Epoch 5/10000\n",
            "313/313 [==============================] - 20s 65ms/step - loss: 0.1577 - accuracy: 0.9443 - val_loss: 0.6391 - val_accuracy: 0.8056\n",
            "Epoch 6/10000\n",
            "313/313 [==============================] - 18s 59ms/step - loss: 0.1301 - accuracy: 0.9557 - val_loss: 0.4974 - val_accuracy: 0.8308\n",
            "Epoch 7/10000\n",
            "313/313 [==============================] - 17s 55ms/step - loss: 0.1052 - accuracy: 0.9647 - val_loss: 0.6322 - val_accuracy: 0.7966\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff3b4beeb60>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# validation을 통해 테스트셋에서의 성능을 가늠할 수 있습니다.\n",
        "# 훈련데이터에 대한 성능은 개선되는데 validation에서 악화된다면 과적합이 일어나는 것일 수도 있습니다.\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# TODO: 자유롭게 설정, restore_best_weights=True 추천\n",
        "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(x_train_pad, y_train, callbacks=es,\n",
        "          validation_split=0.2, epochs=10000, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "382P8ZGMt2Wa"
      },
      "source": [
        "## Performance on Test-Set\n",
        "\n",
        "Now that the model has been trained we can calculate its classification accuracy on the test-set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hP1eEs8xt2Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c2ea306-1555-48a7-cbb6-39757971b1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 10s 12ms/step - loss: 0.3417 - accuracy: 0.8586\n",
            "CPU times: user 6.61 s, sys: 285 ms, total: 6.9 s\n",
            "Wall time: 10.3 s\n"
          ]
        }
      ],
      "source": [
        "# 테스트셋에서의 성능 평가\n",
        "%%time\n",
        "result = model.evaluate(x_test_pad, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "McKlnypZt2Wa",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d0eefe1-c3a7-4f00-d2dc-f64281874dbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.86%\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy: {0:.2%}\".format(result[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alpR4ZXut2Wc"
      },
      "source": [
        "## New Data\n",
        "\n",
        "Let us try and classify new texts that we make up. Some of these are obvious, while others use negation and sarcasm to try and confuse the model into mis-classifying the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7dguFHY5t2Wc"
      },
      "outputs": [],
      "source": [
        "# 새로운 데이터셋 8개 생성하여 texts에 저장\n",
        "text1 = \"This movie is fantastic! I really like it because it is so good!\"\n",
        "text2 = \"Good movie!\"\n",
        "text3 = \"Maybe I like this movie.\"\n",
        "text4 = \"Meh ...\"\n",
        "text5 = \"If I were a drunk teenager then this movie might be good.\"\n",
        "text6 = \"Bad movie!\"\n",
        "text7 = \"Not a good movie!\"\n",
        "text8 = \"This movie really sucks! Can I get my money back please?\"\n",
        "texts = [text1, text2, text3, text4, text5, text6, text7, text8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L21c2SlFt2Wc"
      },
      "source": [
        "We first convert these texts to arrays of integer-tokens because that is needed by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "N53OFtJxt2Wd"
      },
      "outputs": [],
      "source": [
        "# TODO: tokenizer로 texts를 정수로 변환하세요.\n",
        "tokens = tokenizer.texts_to_sequences(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tX0cSgCt2Wd"
      },
      "source": [
        "To input texts with different lengths into the model, we also need to pad and truncate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "gCZytuX-t2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68cddc14-98fe-4e9b-f229-e2babd3bb443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-1e6d27215946>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tokens=np.array(tokens)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 544)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# TODO: 패딩으로 tokens의 길이를 맞춰주세요.\n",
        "tokens=np.array(tokens)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, \n",
        "                            padding=pad, truncating=pad) \n",
        "tokens_pad.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPwl4x3nt2Wd"
      },
      "source": [
        "We can now use the trained model to predict the sentiment for these texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5smpP0MOt2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75f2349-ed6e-4223-f0e7-2e0f63f93f98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9803249 ],\n",
              "       [0.98311275],\n",
              "       [0.9459022 ],\n",
              "       [0.9864442 ],\n",
              "       [0.8933322 ],\n",
              "       [0.9064835 ],\n",
              "       [0.97467935],\n",
              "       [0.8113174 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# TODO: model로 tokens_pad를 예측해보세요.\n",
        "# 1에 가까울 수록 긍정적이고 0에 가까울 수록 부정적입니다.\n",
        "# 잘 예측했나요? 운에 따라 결과가 다를 수 있습니다.\n",
        "model.predict(tokens_pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYhK-DEot2Wd"
      },
      "source": [
        "A value close to 0.0 means a negative sentiment and a value close to 1.0 means a positive sentiment. These numbers will vary every time you train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEL_WlY7HzWR"
      },
      "source": [
        "## Newer Data\n",
        "우리 모델은 평균 200개 이상의 단어가 쓰인 리뷰로 학습됐습니다. 위와 같이 매우 짧은 리뷰에 대해서는 잘 동작하지 않을 수도 있으니 <br>IMDb에서 가디언즈 오브 갤럭시 3의 10점짜리 리뷰와 5점 이하의 리뷰로 테스트 해봅시다.<br>\n",
        "https://www.imdb.com/title/tt6791350/reviews?ref_=tt_urv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_Jqjcb2sH0zG"
      },
      "outputs": [],
      "source": [
        "text1 = \"It all leads back to where we once started off as all great trilogies have indicated from the past. But GOTG surpasses expectations with what is nothing short of phenomenal performances by the cast and truly well written and does not shy away from what could only be understood from what requires a sense of maturity to understand. Rockets back story alone is on occasion dark and incredibly sad it had me in tears at a few instances during the film. Honestly I did not expect this, James Gunn yet again provides an absolutely beautiful piece of cinematography and I have to say I really hope in the future they dive back into the story of the colony . With Adam warlock finally in the MCU it's most likely there will be some cameos. But it does deliver in the traditional guardians humor but this film is meant about more for the mature audience of marvel. Trust me this is a great film.\"#TODO: 10점짜리 리뷰, 제목 제외 본문만 붙여넣으세요.\n",
        "text2 = \"I'm not one to cry at movies often, but this one broke me. Four is merely an estimate. I spent the entire last hour of the film in and out of tears. Guardians of the Galaxy Vol. 3 is an unexpected masterpiece, and a work of art that absolutely broke me.From the soundtrack, to the writing, to the characters, to the use of flashbacks, this movie is about as close to perfection as it gets. Having already established all of the characters, it managed to be heart-wrenching all the way through. This movie leaves a bittersweet taste in your mouth, and with so much to think about. It manages to make 2 and 1/2 hours feel ten times as short. I never expected to fall in love with this movie, but it got to me in such a visceral way that I'll never forget the experience of seeing it.\"#TODO: 10점짜리 리뷰, 제목 제외 본문만 붙여넣으세요.\n",
        "text3 = \"As a big fun of Guardian franchise, I have to say this one is boring, annoying, and unnecessarily childish. I just wish to get out the theater halfway through the movie. Maybe the animal protection theme is the reason why it is rated so high. The movie can be easily made into a 90 minutes version. The constant shouting and anger serve no purpose but to annoy audiences. The comedy is at least two grades down from the previous series. To be fair, I think this one should be rated about the same as the latest Thor movie at around 6.5. But since so many people give such absurd high rating, I feel the need to balance it.\"#TODO: 5점 이하 리뷰, 제목 제외 본문만 붙여넣으세요.\n",
        "text4 = \"I get the idea, but the extreme violence to animals and the overly disgusting detailing killed the entire story of Guardians of The Galaxy for me. It's a 90% tearjerker and 9% of plot and other things and 1% of much awaited humor, which is only funny compared to all the rest of mind numbing sadness of the film. I can't believe its rating. I would not ever recommend it to anyone, who watched and loved the first two. If I could turn back time I wouldn't even attempt to watch it. I regret going to the theater. I regret watching it to the end. I used to call Guardians my favorite Marvel movies, I used to leave the theater overwhelmed with joy, willing to watch it again right away, tell everyone to watch it, listening to soundtracks. Now I'm leaving the theater in tears, feeling hurt and betrayed.\"#TODO: 5점 이하 리뷰, 제목 제외 본문만 붙여넣으세요.\n",
        "texts = [text1, text2, text3, text4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "VN2wWdHDJJNS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d66111-bfee-41fa-bfba-3e072b3c43ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-f5b26b347c47>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  tokens=np.array(tokens)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9740772 ],\n",
              "       [0.89582926],\n",
              "       [0.14475904],\n",
              "       [0.8581828 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "#TODO: 상기 texts에 대해 예측해보세요.\n",
        "#Hint: tokenizer와 padding을 이용해야 합니다.\n",
        "tokens = tokenizer.texts_to_sequences(texts)\n",
        "tokens=np.array(tokens)\n",
        "tokens_pad = pad_sequences(tokens, maxlen=max_tokens, \n",
        "                            padding=pad, truncating=pad) \n",
        "model.predict(tokens_pad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THYTOZSet2Wd"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "임베딩을 통해 각 단어가 어떻게 변환되는지 살펴봅시다.\n",
        "\n",
        "The model cannot work on integer-tokens directly, because they are integer values that may range between 0 and the number of words in our vocabulary, e.g. 10000. So we need to convert the integer-tokens into vectors of values that are roughly between -1.0 and 1.0 which can be used as input to a neural network.\n",
        "\n",
        "This mapping from integer-tokens to real-valued vectors is also called an \"embedding\". It is essentially just a matrix where each row contains the vector-mapping of a single token. This means we can quickly lookup the mapping of each integer-token by simply using the token as an index into the matrix. The embeddings are learned along with the rest of the model during training.\n",
        "\n",
        "Ideally the embedding would learn a mapping where words that are similar in meaning also have similar embedding-values. Let us investigate if that has happened here.\n",
        "\n",
        "First we need to get the embedding-layer from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "jtyAVJbpt2Wd"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층 불러오기\n",
        "layer_embedding = model.get_layer('layer_embedding')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jANfbGKt2Wd"
      },
      "source": [
        "We can then get the weights used for the mapping done by the embedding-layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "uVO-XSXrt2We"
      },
      "outputs": [],
      "source": [
        "# 임베딩 층의 가중치 불러오기\n",
        "weights_embedding = layer_embedding.get_weights()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tO6qnaB0t2We"
      },
      "source": [
        "Note that the weights are actually just a matrix with the number of words in the vocabulary times the vector length for each embedding. That's because it is basically just a lookup-matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-YcE3C_kt2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92631ad4-ae61-4796-bc2a-80d3cc358a73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "# 10,000개의 단어를 8차원으로 보내고 있습니다.\n",
        "weights_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwqGhFLVt2We"
      },
      "source": [
        "Let us get the integer-token for the word 'good', which is just an index into the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "M9xEoo2Ht2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0063378-36c4-4896-87a3-12ad17abde3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "# tokenizer에서 good의 위치\n",
        "token_good = tokenizer.word_index['good']\n",
        "token_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec1XR9Gut2We"
      },
      "source": [
        "Let us also get the integer-token for the word 'great'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "fFz5EWFPt2We",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27488364-c79a-458f-acf5-c787288ebbc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "# tokenizer에서 great의 위치\n",
        "token_great = tokenizer.word_index['great']\n",
        "token_great"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1itM760At2Wf"
      },
      "source": [
        "These integertokens may be far apart and will depend on the frequency of those words in the data-set.\n",
        "\n",
        "Now let us compare the vector-embeddings for the words 'good' and 'great'. Several of these values are similar, although some values are quite different. Note that these values will change every time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ju_7MpFrt2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1054fa6-6334-4224-9359-d0e224d124ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03682254,  0.01327146,  0.07466787, -0.0082793 , -0.01135896,\n",
              "        0.0802061 , -0.07556771,  0.06590198], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# good의 임베딩 결과\n",
        "weights_embedding[token_good]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "K4aw3__At2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d9f57c1-b742-4564-875a-f7e0e005d886"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.05684192, -0.13125876,  0.15607324,  0.14094791,  0.18695542,\n",
              "        0.19273515, -0.12545308,  0.14916804], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# great의 임베딩 결과\n",
        "weights_embedding[token_great]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrbHzmFlt2Wf"
      },
      "source": [
        "Similarly, we can compare the embeddings for the words 'bad' and 'horrible'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "WYOkp0DIt2Wf"
      },
      "outputs": [],
      "source": [
        "# tokenizer에서 bad와 horrible\n",
        "token_bad = tokenizer.word_index['bad']\n",
        "token_horrible = tokenizer.word_index['horrible']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "DhfbcEnit2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd18b4a3-beaf-46ef-8381-3860c6b5c11e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.07445331,  0.05394237, -0.14835995, -0.11048789, -0.08985776,\n",
              "       -0.08476102,  0.12966895, -0.09467645], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# TODO: bad의 임베딩 결과\n",
        "weights_embedding[token_bad]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "8uqa8Krzt2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a35ab41-4147-4ae3-9aec-c44fab6df978"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.15708783,  0.18183774, -0.10485482, -0.18666396, -0.1373758 ,\n",
              "       -0.17384727,  0.13414288, -0.1468237 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "# TODO: horrible의 임베딩 결과\n",
        "weights_embedding[token_horrible]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zOsin_OXnsU"
      },
      "source": [
        "임베딩 결과를 보면 good과 great은 비슷한 곳에 위치하고 good과 bad는 반대되는 곳에 위치한 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gDSIg26t2Wf"
      },
      "source": [
        "### Sorted Words\n",
        "\n",
        "코사인 유사도로 임베딩 공간에서 비슷한 단어를 찾아봅시다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "YyhM3RL0t2Wf"
      },
      "outputs": [],
      "source": [
        "# 주어진 단어와 비슷한 단어를 찾아주는 함수\n",
        "def print_sorted_words(word, metric='cosine'):\n",
        "    \"\"\"\n",
        "    Print the words in the vocabulary sorted according to their\n",
        "    embedding-distance to the given word.\n",
        "    Different metrics can be used, e.g. 'cosine' or 'euclidean'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get the token (i.e. integer ID) for the given word.\n",
        "    token = tokenizer.word_index[word]\n",
        "\n",
        "    # Get the embedding for the given word. Note that the\n",
        "    # embedding-weight-matrix is indexed by the word-tokens\n",
        "    # which are integer IDs.\n",
        "    embedding = weights_embedding[token]\n",
        "\n",
        "    # Calculate the distance between the embeddings for\n",
        "    # this word and all other words in the vocabulary.\n",
        "    distances = cdist(weights_embedding, [embedding],\n",
        "                      metric=metric).T[0]\n",
        "    \n",
        "    # Get an index sorted according to the embedding-distances.\n",
        "    # These are the tokens (integer IDs) for words in the vocabulary.\n",
        "    sorted_index = np.argsort(distances)\n",
        "    \n",
        "    # Sort the embedding-distances.\n",
        "    sorted_distances = distances[sorted_index]\n",
        "    \n",
        "    # Sort all the words in the vocabulary according to their\n",
        "    # embedding-distance. This is a bit excessive because we\n",
        "    # will only print the top and bottom words.\n",
        "    sorted_words = [inverse_map[token] for token in sorted_index\n",
        "                    if token != 0]\n",
        "\n",
        "    # Helper-function for printing words and embedding-distances.\n",
        "    def _print_words(words, distances):\n",
        "        for word, distance in zip(words, distances):\n",
        "            print(\"{0:.3f} - {1}\".format(distance, word))\n",
        "\n",
        "    # Number of words to print from the top and bottom of the list.\n",
        "    k = 10\n",
        "\n",
        "    print(\"Distance from '{0}':\".format(word))\n",
        "\n",
        "    # Print the words with smallest embedding-distance.\n",
        "    _print_words(sorted_words[0:k], sorted_distances[0:k])\n",
        "\n",
        "    print(\"...\")\n",
        "\n",
        "    # Print the words with highest embedding-distance.\n",
        "    _print_words(sorted_words[-k:], sorted_distances[-k:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewbOIavTt2Wg"
      },
      "source": [
        "We can then print the words that are near and far from the word 'great' in terms of their vector-embeddings. Note that these may change each time you train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "CFU1vStzt2Wg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcccda23-06b5-44e0-d8b5-8d5ebfcb04e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'great':\n",
            "0.000 - great\n",
            "0.008 - language\n",
            "0.010 - liz\n",
            "0.012 - relax\n",
            "0.013 - faults\n",
            "0.016 - gorgeous\n",
            "0.018 - mistaken\n",
            "0.018 - nerdy\n",
            "0.019 - realistic\n",
            "0.019 - suspenseful\n",
            "...\n",
            "1.983 - pay\n",
            "1.984 - stupidity\n",
            "1.986 - shameless\n",
            "1.986 - malcolm\n",
            "1.987 - trendy\n",
            "1.988 - stupid\n",
            "1.989 - write\n",
            "1.989 - cardboard\n",
            "1.992 - substance\n",
            "1.992 - rational\n"
          ]
        }
      ],
      "source": [
        "# great와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('great', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZi34Trt2Wg"
      },
      "source": [
        "Similarly, we can print the words that are near and far from the word 'worst' in terms of their vector-embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "P3qmBJ95t2Wg",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cea3658-e913-4827-a106-be9b4829af8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance from 'worst':\n",
            "0.000 - worst\n",
            "0.003 - baldwin\n",
            "0.005 - wasted\n",
            "0.005 - waste\n",
            "0.008 - skip\n",
            "0.009 - prevents\n",
            "0.009 - vinnie\n",
            "0.009 - mess\n",
            "0.011 - badly\n",
            "0.011 - pathetic\n",
            "...\n",
            "1.988 - interpretations\n",
            "1.988 - mistaken\n",
            "1.989 - important\n",
            "1.990 - rukh\n",
            "1.990 - highly\n",
            "1.990 - rare\n",
            "1.991 - relax\n",
            "1.992 - montana\n",
            "1.993 - appreciate\n",
            "1.998 - perfect\n"
          ]
        }
      ],
      "source": [
        "# TODO: worst와 비슷한 단어 10개, 반대 단어 10개\n",
        "print_sorted_words('worst', metric='cosine')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdkL2Jubt2Wg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "감성 분석에서 순환 신경망은 비교적 만족스러운 성능을 보이지만 사람과는 전혀 다른 방식으로 감정을 계산해냅니다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnclb77Ct2Wg"
      },
      "source": [
        "## 연습문제 (수행할 필요 없습니다.)\n",
        "\n",
        "수행할 때마다 다른 결과를 얻을 수 있습니다.\n",
        "\n",
        "* 현재 Tokenizer가 가장 자주 쓰이는 10,000개의 단어를 처리하는데 5000개만 사용하면 성능이 어떻게 변할까요? \n",
        "* 임베딩을 8차원에서 수행하는데 200차원으로 늘리면 성능이 어떻게 변할까요?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTjDZ271t2Wg"
      },
      "source": [
        "## License (MIT)\n",
        "\n",
        "Copyright (c) 2022 by uramoon@kw.ac.kr<br>\n",
        "Copyright (c) 2018 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [
        "ZdkL2Jubt2Wg",
        "wnclb77Ct2Wg",
        "NTjDZ271t2Wg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}